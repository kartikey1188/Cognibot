Namaste welcome to the next video 
of machine learning practice course.   In this video we will discuss the remaining 
steps of end-to-end machine learning project.   We will begin by discussing the step that 
involves fine tuning of machine learning model.   Usually there are a number of hyper parameters 
in machine learning models which are set manually   by the machine learning model developer.
Tuning these hyper parameters usually lead   to better accuracy of machine learning models. 
However finding the best combination of hyper   parameters in a brute force manner is a 
challenging task. The hyperparameter space   is huge and finding the best combination is a 
search problem in this huge parameter space.   Fortunately we have couple of ways in which we can 
perform this activity in scikit-learn package.
  The first one is the grid search. We have a class 
GridSearchCV that helps us in finding the best   combination of hyper parameters. We can import 
the GridSearchCV from model selection package.   We need to specify a list of hyper parameters 
along with the range of values to try.   Once we specify these values GridSearchCV 
would automatically evaluate all possible   combinations of hyper parameters 
using cross validation.
  Let us take a concrete examples example of 
performing hyper parameter search in random   forest regression problem. There are number of 
hyper parameters in random forest regression   such as number of estimators maximum number of 
features. So, we specify the parameter grid or   the combinations of parameters to 
try in form of a parameter grid.   In this case we specify number of estimators to 
try to be a list of three numbers 3 10 and 30   and maximum features that we 
want to try is 2, 4, 6 and 8. 
  We also specify the value of bootstrap to 
be false and the number of estimator to be   3 and 10 and maximum features to be 2, 3 and 4. 
So, we specify here two sets of hyper parameter   hyper parameter search problems. There are 
two combination the first combination contains   number of estimators with three values 
and maximum features with four values   and the second combination has an additional 
bootstrap parameter which is set to false.
  Let us compute the total number of 
combinations that will be evaluated here.   The first one has three values for number 
of estimators and four values for maximum   number of parameters that would lead to 12 
combinations and the second one has two values   of number of estimators three values of maximum 
features thus resulting into 6 combinations.   So, the total number of combinations that 
will be evaluated by the parameter gear   grid is the sum of these two combinations which 
is 12 plus 6 leading to 18 combinations.
  Let us create a GridSearchCV object with the 
parameter grid in GridSearchCV. We specify the   estimator which is forest underscore reg this is 
the random forest regressor object the parameter   grade number of cross validation fold, the 
the scoring the scoring scheme and a flag   that that specifies whether we want to 
also return the training scores.
  So, we need to train this we need to train this 
model for 18 parameter combination and each   combination would be trained five times because 
the number of cross validation is set to 5.   So, in all will be performing 
90 model training runs.
  So, we can launch the hyper parameter search by 
calling fit function on the GridSearchCV.
  Once the once the fit is complete we get 
the best combination of hyperparameter   with the best underscore parameter 
underscore member variable of the object.   Here the best the best combination of parameter 
seems to be maximum features equal to 6 and   number of estimators equal to 30. Let us find 
out errors at different parameter setting   we can find it out with CV underscore 
result underscore member variable.
  And you can see that these are the the mean 
squared error and this is a combination of hyper   parameters. So, in the first row you can see that 
the maximum number of features to be used is 2   and number of estimators to try is 3. In 
this setup we get mean square error of 0.5   and you can see that there are these 18 
combinations that we have listed over here and for   each of these combination we have specified what 
is exactly the value of the hyper parameters that   were tried and what is the mean squared error.
And you can see that the best mean squared error   or the least mean square error is obtained for 
a combination of parameter where max feature   equal to 6 and number of estimator equal to 30 
and that value is 0.352. So, this is the best   parameter this is the best parameter setting 
that was obtained with the grid search. 
  We can obtain the best parameter 
we can obtain the best estimator   with best underscore estimator underscore 
member variable of the grid search object.   And you can see that in this best estimator 
maximum features is equal to set to 6 and   number of estimator is set to 30 and this was the 
combination that was obtained with GridSearchCV.   In this case we have initialized the 
GridSearchCV with refit equal to true option.
  When we set refit equal to true the GridSearhCV 
retains the best estimator on the full training   set this is likely to lead us to a better model 
as it is trained on a larger dataset.
  The second option that is provided by 
the sqln package is randomized search.   When we have a large parameter space 
GridSearchCV can be inefficient   in that case it is desirable to 
try RandomizedSearchCV class.   RandomizedSearchCV class selects a random value 
for each hyper parameter at the start of each   iteration and then it repeats the process for 
the given number of random combinations.
  It enables us to search hyper parameter 
space with appropriate budget control.   So, we can import the RandomizedSearchCV 
class from model selection. 
  The analysis of model provides useful insights 
about the feature. Let us obtain the feature   importance as learned by the model. So, 
with grid search we know that the best   estimator can be obtained with best under best 
underscore estimator underscore member variable.   We can obtain the feature importance by calling 
the member variable of this best estimator.
  Here you can see the future importance 
of each of the variable and we have   listed the future importance in the 
descending order of the importance.   So, the wine quality is is highly dependent 
on the alcohol so or the alcohol feature   has is the most important feature in determining 
the wine quality and it has got score of 0.24   followed by sulphate volatile acidity followed 
by total sulfur dioxide density and so on.
  Based on this information we may draw a 
few features that are not so important   in prediction of the target variable.   And based on the reduced feature set we again 
retrain the model and follow the process we   retrain the model we find the best combination 
of the hyper parameters using either GridSearchCV   or RandomizedSearchCV depending on the 
depending on the size of the parameter space.
  And once we obtain the best 
parameter we can analyze that model   and find the insights about the about 
the features that are important.   It may also be useful to analyze the errors made 
by the model in prediction and understand its   causes. This helps us this helps us in getting 
useful insights and maybe we can go back to the   domain experts and consult if we can add 
better features to stem those errors.
  So, you can see that this machine learning model 
development is totally an iterative process.   We start with some feature set and then we train 
the model we we perform the hyper parameter search   and then we analyze the analyze the 
model. We understand the future importance   and also look at the errors in the model. 
Based on the errors we come up with we go   back to the experts consult them find out 
how to how to take care of the error.
  In order to take care of the error we might need 
a new set of features or some kind of a feature   transformation of the existing existing features 
we perform that and we again go back and retrain   the model. And this process continues until we 
get the model of the satisfactory quality.
  And once we are happy with the model 
performance once we are happy with the   model that we obtained we you know we evaluate its 
performance on the test set. Remember we always   we always report the performance 
of the model on the test set   because ultimately the model has to 
work well it has to generalize well   on the unseen data. Hence the performance 
of the model is reported on the test set.
  While evaluating the performance of the model 
on the test set we need to make sure that the   transformations that were applied on the training 
set are uniformly applied to the test set. Here we   make use of the pipeline class for making sure 
that the transformations are applied uniformly   across both the training and the test set. So we 
use the fit transfer method of the pipeline object   on the on the test set and we obtain the 
the transform version of the test set.
  We use this transform version and call the predict 
method on the best estimator obtained to the grid   search or to there or through the randomized 
search. And this predict method returns the   predictions on the test set. Based on these 
predictions we compare these predictions with   the actual predictions and calculate the mean 
squared error or any other appropriate metric   based on the the model at hand.
In this case we get mean squared error of 0.35.   It is a good idea to get 95 confidence interval 
of the evaluation metric. It can be obtained   by using stats stats class from cycle from 
scipy. We get we get the 95% confidence interval   with stats dot interval by specifying the 
confidence interval and the confidence interval   95% confidence interval for mean square error 
in this case is between 0.29 and 0.41.
  Once we have a reasonable model based on 
its performance on the test set we reach   the pre-launch phase. Before launch we need to 
present our solution that highlights learning   assumptions and system limitations 
to the group of our collaborator.   As we discussed earlier machine learning 
model development is a collaborative activity   between the model developer, domain 
experts and product teams.
  We need to document everything create clear 
visualizations and we should present our model.   In case the model does not work better than 
the experts it may still be a good idea to   launch it this will help us in freeing up 
precious bandwidth of the human experts.
  And the final step in the process is the launch 
step. In launch we plug in the input sources   and write test cases to make sure that the model 
works well end to end. Writing test cases is quite   crucial because it gives us more confidence 
about working of different pieces together.   And once the model is launched we need to 
monitor it closely. We need to monitor for   the system outages find ways to to mitigate 
those outages or minimize those outages.
  While monitoring the model if we found that the 
model performance has degraded we need to have a   strategy to tackle that situation. The situation 
can be tackled either by retraining the model   and this retraining can be scheduled after after 
some kind of a fixed time we can retrain the model   let us say in every two weeks or whenever 
we see a drop in performance by ah by some   kind of a threshold metric then you know we 
can launch the retraining of the model.
  So, for that we need to first fix 
what is the threshold on the metric?   And once that threshold is breached or once 
that threshold is crossed you know model   performance once that threshold is crossed we 
need to trigger the retraining of the model.   Another important thing to evaluate 
the model performance is to sample   predictions for human evaluation. We can we can 
present predictions by the model in production   to the human experts and get their 
feedback on how model is performing.
  We also need to carry out regular assessment 
of the data quality which is critical for   model performance. Remember the model performance 
depends on the feature values and if features are   not getting captured correctly it will adversely 
affect the model performance and the readings at   an under and output that we get 
from the model if we consume   in the downstream tasks there could be there could 
be uh effect that are that are undesirable.
  Finally model maintenance we can 
either train the model regularly   in every fixed interval with fresh data or 
we can also trigger the model retraining   based on some kind of a threshold in the 
in the degradation of model performance.   And we should also figure out how to how to push 
the trained model to production without disrupting   the without disrupting the live 
path of the of the survey.
  So, these were the eight steps involved 
in end-to-end machine learning project.   So, I hope you have a better clarity about 
steps that are involved in machine learning   machine learning project. I request you 
to follow this advice meticulously in   in the subsequent classes or in subsequent 
sessions of this particular course and also you   should carry this advice into your working.
So, if you follow this advice rigorously   you know you will be able to build 
machine learning models of high quality   and you will be a successful machine learning 
developer. I hope you enjoyed this content and and   and in the next and from the next class we will 
start talking about you know mechanics of scale   and library. We start with high level introduction 
to sklearn library and we will deep dive into   different machine learning algorithms and their 
implementations with sklearn library. Namaste. 